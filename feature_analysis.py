# feature_analysis.py - ç‰¹å¾å·¥ç¨‹åˆ†æè„šæœ¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_regression, mutual_info_classif
from sklearn.preprocessing import StandardScaler
from technical_indicators import add_technical_indicators, get_feature_importance_analysis
from preprocess import load_and_preprocess
from config_improved import *

def analyze_features():
    """åˆ†æç‰¹å¾å·¥ç¨‹æ•ˆæœ"""
    print("=== ç‰¹å¾å·¥ç¨‹åˆ†æ ===")
    
    # åŠ è½½æ•°æ®
    df = load_and_preprocess(DATA_PATH)
    df = add_technical_indicators(df)
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"ç‰¹å¾æ•°é‡: {len(FEATURE_COLUMNS)}")
    
    # åŸºç¡€ç»Ÿè®¡åˆ†æ
    print("\n=== åŸºç¡€ç»Ÿè®¡åˆ†æ ===")
    feature_stats = df[FEATURE_COLUMNS].describe()
    print(feature_stats)
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    print("\n=== ç¼ºå¤±å€¼æ£€æŸ¥ ===")
    missing_values = df[FEATURE_COLUMNS].isnull().sum()
    print(missing_values)
    
    # æ£€æŸ¥æ— ç©·å¤§å€¼
    print("\n=== æ— ç©·å¤§å€¼æ£€æŸ¥ ===")
    inf_values = np.isinf(df[FEATURE_COLUMNS]).sum()
    print(inf_values)
    
    # ç‰¹å¾ç›¸å…³æ€§åˆ†æ
    print("\n=== ç‰¹å¾ç›¸å…³æ€§åˆ†æ ===")
    correlation_analysis(df)
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\n=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")
    importance_analysis(df)
    
    # ç‰¹å¾ç¨³å®šæ€§åˆ†æ
    print("\n=== ç‰¹å¾ç¨³å®šæ€§åˆ†æ ===")
    stability_analysis(df)
    
    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    print("\n=== ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š ===")
    generate_visualizations(df)

def correlation_analysis(df: pd.DataFrame):
    """ç‰¹å¾ç›¸å…³æ€§åˆ†æ"""
    feature_df = df[FEATURE_COLUMNS].copy()
    
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    corr_matrix = feature_df.corr()
    
    # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„ç‰¹å¾å¯¹
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = corr_matrix.iloc[i, j]
            if abs(corr_value) > 0.8:
                high_corr_pairs.append({
                    'feature1': corr_matrix.columns[i],
                    'feature2': corr_matrix.columns[j],
                    'correlation': corr_value
                })
    
    print(f"å‘ç° {len(high_corr_pairs)} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾ (|r| > 0.8):")
    for pair in high_corr_pairs:
        print(f"  {pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.3f}")
    
    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç›¸å…³æ€§
    avg_corr = corr_matrix.abs().mean().sort_values(ascending=False)
    print(f"\nç‰¹å¾å¹³å‡ç›¸å…³æ€§ (é™åº):")
    for feature, corr in avg_corr.items():
        print(f"  {feature}: {corr:.3f}")

def importance_analysis(df: pd.DataFrame):
    """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
    feature_df = df[FEATURE_COLUMNS].copy()
    
    # åˆ›å»ºç›®æ ‡å˜é‡
    if USE_CLASSIFICATION:
        # åˆ†ç±»ç›®æ ‡
        target = create_classification_target(df)
        mi_scores = mutual_info_classif(feature_df, target, random_state=42)
    else:
        # å›å½’ç›®æ ‡
        target = create_regression_target(df)
        mi_scores = mutual_info_regression(feature_df, target, random_state=42)
    
    # è®¡ç®—ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': FEATURE_COLUMNS,
        'importance': mi_scores
    }).sort_values('importance', ascending=False)
    
    print("ç‰¹å¾é‡è¦æ€§ (äº’ä¿¡æ¯):")
    for _, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")
    
    # è¯†åˆ«ä½é‡è¦æ€§ç‰¹å¾
    low_importance_threshold = 0.01
    low_importance_features = feature_importance[feature_importance['importance'] < low_importance_threshold]
    
    if len(low_importance_features) > 0:
        print(f"\nâš ï¸ å‘ç° {len(low_importance_features)} ä¸ªä½é‡è¦æ€§ç‰¹å¾ (< {low_importance_threshold}):")
        for _, row in low_importance_features.iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f}")

def stability_analysis(df: pd.DataFrame):
    """ç‰¹å¾ç¨³å®šæ€§åˆ†æ"""
    feature_df = df[FEATURE_COLUMNS].copy()
    
    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ–¹å·®
    feature_variance = feature_df.var().sort_values(ascending=False)
    
    print("ç‰¹å¾æ–¹å·® (é™åº):")
    for feature, variance in feature_variance.items():
        print(f"  {feature}: {variance:.6f}")
    
    # è¯†åˆ«ä½æ–¹å·®ç‰¹å¾
    low_variance_threshold = feature_variance.quantile(0.1)  # æœ€ä½10%åˆ†ä½æ•°
    low_variance_features = feature_variance[feature_variance < low_variance_threshold]
    
    if len(low_variance_features) > 0:
        print(f"\nâš ï¸ å‘ç° {len(low_variance_features)} ä¸ªä½æ–¹å·®ç‰¹å¾ (< {low_variance_threshold:.6f}):")
        for feature, variance in low_variance_features.items():
            print(f"  {feature}: {variance:.6f}")
    
    # è®¡ç®—ç‰¹å¾çš„æ—¶é—´ç¨³å®šæ€§
    print(f"\n=== ç‰¹å¾æ—¶é—´ç¨³å®šæ€§åˆ†æ ===")
    stability_scores = calculate_temporal_stability(df)
    
    print("ç‰¹å¾æ—¶é—´ç¨³å®šæ€§ (è¶Šé«˜è¶Šç¨³å®š):")
    for feature, stability in stability_scores.items():
        print(f"  {feature}: {stability:.3f}")

def calculate_temporal_stability(df: pd.DataFrame) -> dict:
    """è®¡ç®—ç‰¹å¾çš„æ—¶é—´ç¨³å®šæ€§"""
    stability_scores = {}
    
    for feature in FEATURE_COLUMNS:
        if feature in df.columns:
            # è®¡ç®—æ»šåŠ¨çª—å£å†…çš„æ ‡å‡†å·®
            rolling_std = df[feature].rolling(window=50).std()
            # ç¨³å®šæ€§ = 1 / (1 + å¹³å‡æ»šåŠ¨æ ‡å‡†å·®)
            avg_rolling_std = rolling_std.mean()
            stability = 1 / (1 + avg_rolling_std) if avg_rolling_std > 0 else 1
            stability_scores[feature] = stability
    
    return dict(sorted(stability_scores.items(), key=lambda x: x[1], reverse=True))

def create_classification_target(df: pd.DataFrame) -> np.ndarray:
    """åˆ›å»ºåˆ†ç±»ç›®æ ‡å˜é‡"""
    targets = []
    threshold = CLASSIFICATION_THRESHOLD
    
    for i in range(WINDOW_SIZE, len(df)):
        current_close = df.iloc[i-1]['close']
        next_close = df.iloc[i]['close']
        change_ratio = (next_close - current_close) / current_close
        
        if change_ratio < -threshold:
            target = 0  # è·Œ
        elif change_ratio > threshold:
            target = 2  # æ¶¨
        else:
            target = 1  # å¹³
        
        targets.append(target)
    
    return np.array(targets)

def create_regression_target(df: pd.DataFrame) -> np.ndarray:
    """åˆ›å»ºå›å½’ç›®æ ‡å˜é‡"""
    targets = []
    
    for i in range(WINDOW_SIZE, len(df)):
        current_close = df.iloc[i-1]['close']
        next_close = df.iloc[i]['close']
        change_ratio = (next_close - current_close) / current_close
        
        # é™åˆ¶å˜åŒ–ç‡èŒƒå›´
        change_ratio = np.clip(change_ratio, -MAX_CHANGE_RATIO, MAX_CHANGE_RATIO)
        
        targets.append(change_ratio)
    
    return np.array(targets)

def generate_visualizations(df: pd.DataFrame):
    """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"""
    try:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ç‰¹å¾å·¥ç¨‹åˆ†ææŠ¥å‘Š', fontsize=16)
        
        # 1. ç›¸å…³æ€§çƒ­åŠ›å›¾
        feature_df = df[FEATURE_COLUMNS].copy()
        corr_matrix = feature_df.corr()
        
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                   square=True, ax=axes[0,0])
        axes[0,0].set_title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')
        axes[0,0].tick_params(axis='x', rotation=45)
        axes[0,0].tick_params(axis='y', rotation=0)
        
        # 2. ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
        if USE_CLASSIFICATION:
            target = create_classification_target(df)
            mi_scores = mutual_info_classif(feature_df, target, random_state=42)
        else:
            target = create_regression_target(df)
            mi_scores = mutual_info_regression(feature_df, target, random_state=42)
        
        feature_importance = pd.DataFrame({
            'feature': FEATURE_COLUMNS,
            'importance': mi_scores
        }).sort_values('importance', ascending=True)
        
        axes[0,1].barh(range(len(feature_importance)), feature_importance['importance'])
        axes[0,1].set_yticks(range(len(feature_importance)))
        axes[0,1].set_yticklabels(feature_importance['feature'])
        axes[0,1].set_title('ç‰¹å¾é‡è¦æ€§ (äº’ä¿¡æ¯)')
        axes[0,1].set_xlabel('é‡è¦æ€§å¾—åˆ†')
        
        # 3. ç‰¹å¾æ–¹å·®åˆ†å¸ƒ
        feature_variance = feature_df.var().sort_values(ascending=True)
        axes[1,0].barh(range(len(feature_variance)), feature_variance.values)
        axes[1,0].set_yticks(range(len(feature_variance)))
        axes[1,0].set_yticklabels(feature_variance.index)
        axes[1,0].set_title('ç‰¹å¾æ–¹å·®åˆ†å¸ƒ')
        axes[1,0].set_xlabel('æ–¹å·®')
        
        # 4. ç‰¹å¾æ—¶é—´åºåˆ—ç¤ºä¾‹ (é€‰æ‹©å‰3ä¸ªé‡è¦ç‰¹å¾)
        top_features = feature_importance.tail(3)['feature'].tolist()
        for i, feature in enumerate(top_features):
            if feature in df.columns:
                # åªæ˜¾ç¤ºæœ€å1000ä¸ªæ•°æ®ç‚¹
                sample_data = df[feature].tail(1000)
                axes[1,1].plot(sample_data.index, sample_data.values, 
                              label=feature, alpha=0.7)
        
        axes[1,1].set_title('é‡è¦ç‰¹å¾æ—¶é—´åºåˆ—ç¤ºä¾‹')
        axes[1,1].set_xlabel('æ—¶é—´ç´¢å¼•')
        axes[1,1].set_ylabel('ç‰¹å¾å€¼')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('feature_analysis_report.png', dpi=300, bbox_inches='tight')
        print("âœ… å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜ä¸º: feature_analysis_report.png")
        
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šå¤±è´¥: {e}")

def recommend_feature_improvements():
    """æ¨èç‰¹å¾æ”¹è¿›å»ºè®®"""
    print("\n=== ç‰¹å¾æ”¹è¿›å»ºè®® ===")
    
    # åŠ è½½æ•°æ®è¿›è¡Œåˆ†æ
    df = load_and_preprocess(DATA_PATH)
    df = add_technical_indicators(df)
    
    # åˆ†æå½“å‰ç‰¹å¾
    feature_analysis = get_feature_importance_analysis(df)
    
    print("ğŸ“Š å½“å‰ç‰¹å¾åˆ†æ:")
    print(f"  æ€»ç‰¹å¾æ•°: {feature_analysis['total_features']}")
    print(f"  é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹: {len(feature_analysis['high_correlation_pairs'])}")
    
    # å»ºè®®
    print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    
    if len(feature_analysis['high_correlation_pairs']) > 0:
        print("  1. ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ä¸­çš„å†—ä½™ç‰¹å¾")
        for pair in feature_analysis['high_correlation_pairs']:
            print(f"     - è€ƒè™‘ç§»é™¤ {pair['feature1']} æˆ– {pair['feature2']} (ç›¸å…³æ€§: {pair['correlation']:.3f})")
    
    # æ£€æŸ¥ä½æ–¹å·®ç‰¹å¾
    low_variance_features = []
    for feature, variance in feature_analysis['feature_variance'].items():
        if variance < 0.001:  # ä½æ–¹å·®é˜ˆå€¼
            low_variance_features.append(feature)
    
    if low_variance_features:
        print(f"  2. è€ƒè™‘ç§»é™¤ä½æ–¹å·®ç‰¹å¾: {low_variance_features}")
    
    print("  3. è€ƒè™‘æ·»åŠ ä»¥ä¸‹æ–°ç‰¹å¾:")
    print("     - ä»·æ ¼åŠ¨é‡ç‰¹å¾ (ä¸åŒæ—¶é—´çª—å£)")
    print("     - æˆäº¤é‡ç›¸å…³ç‰¹å¾")
    print("     - æ³¢åŠ¨ç‡ç‰¹å¾")
    print("     - å¸‚åœºæƒ…ç»ªæŒ‡æ ‡")
    
    print("  4. ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–:")
    print("     - ä½¿ç”¨å¯¹æ•°æ”¶ç›Šç‡æ›¿ä»£åŸå§‹ä»·æ ¼")
    print("     - åº”ç”¨z-scoreæ ‡å‡†åŒ–")
    print("     - è€ƒè™‘ç‰¹å¾é€‰æ‹©ç®—æ³•")

if __name__ == "__main__":
    analyze_features()
    recommend_feature_improvements() 