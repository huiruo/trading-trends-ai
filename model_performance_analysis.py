# model_performance_analysis.py - æ¨¡å‹æ€§èƒ½åˆ†æè„šæœ¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from preprocess import load_and_preprocess, create_sequences
from technical_indicators import add_technical_indicators
from model.model import LSTMModel
from config_improved import *
import torch

def analyze_model_performance():
    """åˆ†ææ¨¡å‹æ€§èƒ½å¹¶æä¾›æ”¹è¿›å»ºè®®"""
    print("=== æ¨¡å‹æ€§èƒ½æ·±åº¦åˆ†æ ===")
    
    # åŠ è½½æ•°æ®
    df = load_and_preprocess(DATA_PATH)
    df = add_technical_indicators(df)
    
    # åˆ›å»ºåºåˆ—
    X, _ = create_sequences(df, window_size=WINDOW_SIZE)
    
    if USE_CLASSIFICATION:
        analyze_classification_performance(X, df)
    else:
        analyze_regression_performance(X, df)

def analyze_classification_performance(X: np.ndarray, df: pd.DataFrame):
    """åˆ†æåˆ†ç±»æ¨¡å‹æ€§èƒ½"""
    print("\n=== åˆ†ç±»æ¨¡å‹æ€§èƒ½åˆ†æ ===")
    
    # åŠ è½½æ¨¡å‹
    model = LSTMModel(
        input_size=X.shape[2], 
        hidden_size=64, 
        num_layers=2, 
        num_classes=NUM_CLASSES
    )
    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))
    model.eval()
    
    # é¢„æµ‹
    predictions = []
    actual_changes = []
    confidences = []
    class_predictions = []
    class_actuals = []
    
    with torch.no_grad():
        for i in range(len(X)):
            x_tensor = torch.tensor(X[i], dtype=torch.float32).unsqueeze(0)
            pred_probs = model(x_tensor)
            pred_class = torch.argmax(pred_probs, dim=1).item()
            confidence = torch.max(pred_probs, dim=1).values.item()
            
            # å®é™…å˜åŒ–
            current_close = df.iloc[i + WINDOW_SIZE - 1]['close']
            actual_close = df.iloc[i + WINDOW_SIZE]['close']
            actual_change_ratio = (actual_close - current_close) / current_close
            
            # å®é™…åˆ†ç±»
            if actual_change_ratio < -CLASSIFICATION_THRESHOLD:
                actual_class = 0  # è·Œ
            elif actual_change_ratio > CLASSIFICATION_THRESHOLD:
                actual_class = 2  # æ¶¨
            else:
                actual_class = 1  # å¹³
            
            predictions.append(pred_class)
            actual_changes.append(actual_change_ratio)
            confidences.append(confidence)
            class_predictions.append(pred_class)
            class_actuals.append(actual_class)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    predictions = np.array(predictions)
    actual_changes = np.array(actual_changes)
    confidences = np.array(confidences)
    class_predictions = np.array(class_predictions)
    class_actuals = np.array(class_actuals)
    
    # 1. åˆ†ç±»åˆ†å¸ƒåˆ†æ
    print("\n1. åˆ†ç±»åˆ†å¸ƒåˆ†æ:")
    pred_dist = np.bincount(predictions, minlength=3)
    actual_dist = np.bincount(class_actuals, minlength=3)
    
    labels = ['è·Œ', 'å¹³', 'æ¶¨']
    for i, label in enumerate(labels):
        print(f"  {label}: é¢„æµ‹={pred_dist[i]} ({pred_dist[i]/len(predictions)*100:.1f}%), "
              f"å®é™…={actual_dist[i]} ({actual_dist[i]/len(class_actuals)*100:.1f}%)")
    
    # 2. ç½®ä¿¡åº¦åˆ†æ
    print("\n2. ç½®ä¿¡åº¦åˆ†æ:")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {confidences.mean():.3f}")
    print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {confidences.std():.3f}")
    print(f"  æœ€é«˜ç½®ä¿¡åº¦: {confidences.max():.3f}")
    print(f"  æœ€ä½ç½®ä¿¡åº¦: {confidences.min():.3f}")
    
    # æŒ‰ç½®ä¿¡åº¦åˆ†ç»„åˆ†æå‡†ç¡®ç‡
    confidence_bins = [0.3, 0.5, 0.7, 0.9, 1.0]
    for i in range(len(confidence_bins)-1):
        mask = (confidences >= confidence_bins[i]) & (confidences < confidence_bins[i+1])
        if mask.sum() > 0:
            accuracy = (predictions[mask] == class_actuals[mask]).mean()
            print(f"  ç½®ä¿¡åº¦ {confidence_bins[i]:.1f}-{confidence_bins[i+1]:.1f}: "
                  f"å‡†ç¡®ç‡={accuracy:.3f} (æ ·æœ¬æ•°={mask.sum()})")
    
    # 3. é”™è¯¯æ¨¡å¼åˆ†æ
    print("\n3. é”™è¯¯æ¨¡å¼åˆ†æ:")
    errors = predictions != class_actuals
    error_indices = np.where(errors)[0]
    
    if len(error_indices) > 0:
        print(f"  æ€»é”™è¯¯æ•°: {len(error_indices)}")
        
        # åˆ†æé”™è¯¯ç±»å‹
        error_types = []
        for idx in error_indices:
            actual = class_actuals[idx]
            pred = predictions[idx]
            if actual == 0 and pred == 2:  # å®é™…è·Œï¼Œé¢„æµ‹æ¶¨
                error_types.append('è·Œ->æ¶¨')
            elif actual == 2 and pred == 0:  # å®é™…æ¶¨ï¼Œé¢„æµ‹è·Œ
                error_types.append('æ¶¨->è·Œ')
            elif actual == 1:  # å®é™…å¹³
                if pred == 0:
                    error_types.append('å¹³->è·Œ')
                else:
                    error_types.append('å¹³->æ¶¨')
            elif pred == 1:  # é¢„æµ‹å¹³
                if actual == 0:
                    error_types.append('è·Œ->å¹³')
                else:
                    error_types.append('æ¶¨->å¹³')
        
        from collections import Counter
        error_counts = Counter(error_types)
        print("  é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
        for error_type, count in error_counts.most_common():
            print(f"    {error_type}: {count} ({count/len(error_indices)*100:.1f}%)")
    
    # 4. æ—¶é—´åºåˆ—åˆ†æ
    print("\n4. æ—¶é—´åºåˆ—åˆ†æ:")
    # åˆ†ææœ€è¿‘100ä¸ªé¢„æµ‹çš„å‡†ç¡®ç‡
    recent_accuracy = (predictions[-100:] == class_actuals[-100:]).mean()
    print(f"  æœ€è¿‘100ä¸ªé¢„æµ‹å‡†ç¡®ç‡: {recent_accuracy:.3f}")
    
    # åˆ†æä¸åŒæ—¶é—´æ®µçš„å‡†ç¡®ç‡
    segments = 5
    segment_size = len(predictions) // segments
    for i in range(segments):
        start_idx = i * segment_size
        end_idx = (i + 1) * segment_size if i < segments - 1 else len(predictions)
        segment_accuracy = (predictions[start_idx:end_idx] == class_actuals[start_idx:end_idx]).mean()
        print(f"  æ—¶é—´æ®µ {i+1}: å‡†ç¡®ç‡={segment_accuracy:.3f}")
    
    # 5. ç”Ÿæˆæ”¹è¿›å»ºè®®
    print("\n5. æ”¹è¿›å»ºè®®:")
    
    # æ£€æŸ¥æ˜¯å¦è¿‡åº¦é¢„æµ‹æŸä¸ªç±»åˆ«
    pred_ratios = pred_dist / len(predictions)
    actual_ratios = actual_dist / len(class_actuals)
    
    for i, label in enumerate(labels):
        ratio_diff = pred_ratios[i] - actual_ratios[i]
        if abs(ratio_diff) > 0.1:  # å·®å¼‚è¶…è¿‡10%
            if ratio_diff > 0:
                print(f"  âš ï¸ è¿‡åº¦é¢„æµ‹{label} (+{ratio_diff*100:.1f}%)")
            else:
                print(f"  âš ï¸ é¢„æµ‹ä¸è¶³{label} ({ratio_diff*100:.1f}%)")
    
    # æ£€æŸ¥ç½®ä¿¡åº¦åˆ†å¸ƒ
    if confidences.mean() < 0.5:
        print("  âš ï¸ å¹³å‡ç½®ä¿¡åº¦è¿‡ä½ï¼Œæ¨¡å‹ä¸å¤Ÿç¡®å®š")
    
    if confidences.std() < 0.1:
        print("  âš ï¸ ç½®ä¿¡åº¦å˜åŒ–å¤ªå°ï¼Œæ¨¡å‹å¯èƒ½è¿‡äºä¿å®ˆ")
    
    # å»ºè®®è°ƒæ•´é˜ˆå€¼
    if pred_dist[1] / len(predictions) > 0.6:  # å¹³ç›˜é¢„æµ‹è¶…è¿‡60%
        print("  ğŸ’¡ å»ºè®®é™ä½åˆ†ç±»é˜ˆå€¼ï¼Œå‡å°‘å¹³ç›˜é¢„æµ‹")
    
    # å»ºè®®å¢åŠ è®­ç»ƒæ•°æ®
    if len(predictions) < 5000:
        print("  ğŸ’¡ å»ºè®®å¢åŠ è®­ç»ƒæ•°æ®é‡")
    
    # 6. ç”Ÿæˆå¯è§†åŒ–
    generate_performance_visualizations(predictions, class_actuals, confidences, actual_changes)

def analyze_regression_performance(X: np.ndarray, df: pd.DataFrame):
    """åˆ†æå›å½’æ¨¡å‹æ€§èƒ½"""
    print("\n=== å›å½’æ¨¡å‹æ€§èƒ½åˆ†æ ===")
    
    # åŠ è½½æ¨¡å‹
    model = LSTMModel(
        input_size=X.shape[2], 
        hidden_size=64, 
        num_layers=2, 
        num_classes=1
    )
    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))
    model.eval()
    
    # é¢„æµ‹
    predictions = []
    actual_changes = []
    
    with torch.no_grad():
        for i in range(len(X)):
            x_tensor = torch.tensor(X[i], dtype=torch.float32).unsqueeze(0)
            pred_normalized = model(x_tensor).item()
            
            # è½¬æ¢é¢„æµ‹å€¼
            pred_change_ratio = (pred_normalized * 2 * MAX_CHANGE_RATIO) - MAX_CHANGE_RATIO
            
            # å®é™…å˜åŒ–
            current_close = df.iloc[i + WINDOW_SIZE - 1]['close']
            actual_close = df.iloc[i + WINDOW_SIZE]['close']
            actual_change_ratio = (actual_close - current_close) / current_close
            
            predictions.append(pred_change_ratio)
            actual_changes.append(actual_change_ratio)
    
    predictions = np.array(predictions)
    actual_changes = np.array(actual_changes)
    
    # åˆ†æé¢„æµ‹è¯¯å·®
    errors = predictions - actual_changes
    
    print(f"\né¢„æµ‹è¯¯å·®åˆ†æ:")
    print(f"  å¹³å‡è¯¯å·®: {errors.mean()*100:.3f}%")
    print(f"  è¯¯å·®æ ‡å‡†å·®: {errors.std()*100:.3f}%")
    print(f"  å¹³å‡ç»å¯¹è¯¯å·®: {np.abs(errors).mean()*100:.3f}%")
    print(f"  æœ€å¤§è¯¯å·®: {errors.max()*100:.3f}%")
    print(f"  æœ€å°è¯¯å·®: {errors.min()*100:.3f}%")
    
    # æ–¹å‘å‡†ç¡®ç‡
    pred_directions = predictions > 0
    actual_directions = actual_changes > 0
    direction_accuracy = (pred_directions == actual_directions).mean()
    print(f"\næ–¹å‘å‡†ç¡®ç‡: {direction_accuracy:.3f}")
    
    # åˆ†æä¸åŒå¹…åº¦åŒºé—´çš„å‡†ç¡®ç‡
    print(f"\nä¸åŒå¹…åº¦åŒºé—´çš„æ–¹å‘å‡†ç¡®ç‡:")
    magnitude_bins = [0.001, 0.005, 0.01, 0.02, 0.05]
    for i in range(len(magnitude_bins)-1):
        mask = (np.abs(actual_changes) >= magnitude_bins[i]) & (np.abs(actual_changes) < magnitude_bins[i+1])
        if mask.sum() > 0:
            accuracy = (pred_directions[mask] == actual_directions[mask]).mean()
            print(f"  {magnitude_bins[i]*100:.1f}%-{magnitude_bins[i+1]*100:.1f}%: "
                  f"å‡†ç¡®ç‡={accuracy:.3f} (æ ·æœ¬æ•°={mask.sum()})")

def generate_performance_visualizations(predictions, actuals, confidences, actual_changes):
    """ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š', fontsize=16)
        
        # 1. æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(actuals, predictions)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['è·Œ', 'å¹³', 'æ¶¨'], 
                   yticklabels=['è·Œ', 'å¹³', 'æ¶¨'], ax=axes[0,0])
        axes[0,0].set_title('æ··æ·†çŸ©é˜µ')
        axes[0,0].set_xlabel('é¢„æµ‹')
        axes[0,0].set_ylabel('å®é™…')
        
        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒ
        axes[0,1].hist(confidences, bins=20, alpha=0.7, edgecolor='black')
        axes[0,1].set_title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        axes[0,1].set_xlabel('ç½®ä¿¡åº¦')
        axes[0,1].set_ylabel('é¢‘æ¬¡')
        axes[0,1].axvline(confidences.mean(), color='red', linestyle='--', 
                          label=f'å¹³å‡: {confidences.mean():.3f}')
        axes[0,1].legend()
        
        # 3. å‡†ç¡®ç‡éšæ—¶é—´å˜åŒ–
        window_size = 100
        accuracies = []
        for i in range(0, len(predictions), window_size):
            end_idx = min(i + window_size, len(predictions))
            accuracy = (predictions[i:end_idx] == actuals[i:end_idx]).mean()
            accuracies.append(accuracy)
        
        axes[1,0].plot(accuracies)
        axes[1,0].set_title('å‡†ç¡®ç‡éšæ—¶é—´å˜åŒ–')
        axes[1,0].set_xlabel('æ—¶é—´çª—å£')
        axes[1,0].set_ylabel('å‡†ç¡®ç‡')
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. å®é™…å˜åŒ–åˆ†å¸ƒ
        axes[1,1].hist(actual_changes * 100, bins=50, alpha=0.7, edgecolor='black')
        axes[1,1].set_title('å®é™…ä»·æ ¼å˜åŒ–åˆ†å¸ƒ')
        axes[1,1].set_xlabel('å˜åŒ–å¹…åº¦ (%)')
        axes[1,1].set_ylabel('é¢‘æ¬¡')
        axes[1,1].axvline(0, color='red', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.savefig('model_performance_analysis.png', dpi=300, bbox_inches='tight')
        print("âœ… æ€§èƒ½åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º: model_performance_analysis.png")
        
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")

if __name__ == "__main__":
    analyze_model_performance() 