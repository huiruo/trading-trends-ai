# model/train_improved.py - ç»Ÿä¸€è®­ç»ƒè„šæœ¬ï¼ˆåˆ†ç±»/å›å½’ï¼‰
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from preprocess import load_and_preprocess, create_sequences
from technical_indicators import add_technical_indicators, get_feature_importance_analysis
from model.model import LSTMModel
from config_improved import *
import numpy as np
import pandas as pd # Added missing import for pandas

def create_labels_classification(df: pd.DataFrame, window_size: int) -> np.ndarray:
    """ä¸ºåˆ†ç±»æ¨¡å‹åˆ›å»ºæ ‡ç­¾"""
    labels = []
    threshold = CLASSIFICATION_THRESHOLD
    
    for i in range(window_size, len(df)):
        current_close = df.iloc[i-1]['close']
        next_close = df.iloc[i]['close']
        change_ratio = (next_close - current_close) / current_close
        
        # 2åˆ†ç±»ï¼šè·Œã€æ¶¨
        if change_ratio < 0:
            label = 0  # è·Œ
        else:
            label = 1  # æ¶¨
        
        labels.append(label)
    
    return np.array(labels)

def create_labels_regression(df: pd.DataFrame, window_size: int) -> np.ndarray:
    """ä¸ºå›å½’æ¨¡å‹åˆ›å»ºæ ‡ç­¾"""
    labels = []
    
    for i in range(window_size, len(df)):
        current_close = df.iloc[i-1]['close']
        next_close = df.iloc[i]['close']
        change_ratio = (next_close - current_close) / current_close
        
        # é™åˆ¶å˜åŒ–ç‡èŒƒå›´
        change_ratio = np.clip(change_ratio, -MAX_CHANGE_RATIO, MAX_CHANGE_RATIO)
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        normalized_change = (change_ratio + MAX_CHANGE_RATIO) / (2 * MAX_CHANGE_RATIO)
        
        labels.append(normalized_change)
    
    return np.array(labels)

def train_improved(csv_path: str, epochs=TRAIN_EPOCHS, lr=LEARNING_RATE, batch_size=BATCH_SIZE):
    """ç»Ÿä¸€è®­ç»ƒå‡½æ•°"""
    
    print("=== æ¨¡å‹è®­ç»ƒå¼€å§‹ ===")
    print(f"æ¨¡å‹ç±»å‹: {'åˆ†ç±»' if USE_CLASSIFICATION else 'å›å½’'}")
    print(f"ç‰¹å¾æ•°é‡: {len(FEATURE_COLUMNS)}")
    print(f"ç‰¹å¾åˆ—è¡¨: {FEATURE_COLUMNS}")
    
    # åŠ è½½æ•°æ®
    df = load_and_preprocess(csv_path)
    
    # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
    df = add_technical_indicators(df)
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    print("\n=== æ•°æ®è´¨é‡æ£€æŸ¥ ===")
    nan_count = df.isna().sum().sum()
    inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()
    print(f"NaNå€¼æ•°é‡: {nan_count}")
    print(f"æ— ç©·å¤§å€¼æ•°é‡: {inf_count}")
    
    if nan_count > 0 or inf_count > 0:
        print("âš ï¸ å‘ç°NaNæˆ–æ— ç©·å¤§å€¼ï¼Œæ­£åœ¨æ¸…ç†...")
        df = df.ffill().bfill()
        df = df.replace([np.inf, -np.inf], 0)
        print("æ•°æ®æ¸…ç†å®Œæˆ")
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\n=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")
    feature_analysis = get_feature_importance_analysis(df)
    print(f"æ€»ç‰¹å¾æ•°: {feature_analysis['total_features']}")
    
    if feature_analysis['high_correlation_pairs']:
        print("âš ï¸ å‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹:")
        for pair in feature_analysis['high_correlation_pairs']:
            print(f"  {pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.3f}")
    
    # åˆ›å»ºåºåˆ—
    X, _ = create_sequences(df, window_size=WINDOW_SIZE)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæ ‡ç­¾
    if USE_CLASSIFICATION:
        y = create_labels_classification(df, WINDOW_SIZE)
        num_classes = NUM_CLASSES
        print(f"\nåˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ:")
        unique, counts = np.unique(y, return_counts=True)
        for i, count in zip(unique, counts):
            label_name = ['è·Œ', 'æ¶¨'][i]
            print(f"  {label_name}: {count} ({count/len(y)*100:.1f}%)")
    else:
        y = create_labels_regression(df, WINDOW_SIZE)
        num_classes = 1
        print(f"\nå›å½’æ ‡ç­¾ç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {y.min():.4f}")
        print(f"  æœ€å¤§å€¼: {y.max():.4f}")
        print(f"  å‡å€¼: {y.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {y.std():.4f}")
    
    print(f"\nè®­ç»ƒæ ·æœ¬æ•°: {len(X)}")
    print(f"ç‰¹å¾ç»´åº¦: {X.shape[2]}")
    
    # åˆ›å»ºæ¨¡å‹
    model = LSTMModel(
        input_size=X.shape[2], 
        hidden_size=64, 
        num_layers=2, 
        num_classes=num_classes
    )
    
    # åˆ é™¤æ—§æ¨¡å‹
    if os.path.exists(MODEL_PATH):
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒ")
        os.remove(MODEL_PATH)
    
    # è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    if USE_CLASSIFICATION:
        loss_fn = nn.CrossEntropyLoss()
        print("ä½¿ç”¨åˆ†ç±»æŸå¤±å‡½æ•°: CrossEntropyLoss")
    else:
        loss_fn = nn.MSELoss()
        print("ä½¿ç”¨å›å½’æŸå¤±å‡½æ•°: MSELoss")
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    # å‡†å¤‡æ•°æ®
    X_tensor = torch.tensor(X, dtype=torch.float32)
    if USE_CLASSIFICATION:
        y_tensor = torch.tensor(y, dtype=torch.long)  # åˆ†ç±»ä½¿ç”¨longç±»å‹
    else:
        y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)  # å›å½’ä½¿ç”¨floatç±»å‹
    
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    patience_counter = 0
    max_patience = 20
    
    print(f"\n=== å¼€å§‹è®­ç»ƒ ===")
    for epoch in range(epochs):
        total_loss = 0
        batch_count = 0
        model.train()
        
        for xb, yb in dataloader:
            # æ£€æŸ¥æ•°æ®è´¨é‡
            if torch.isnan(xb).any() or torch.isnan(yb).any():
                print(f"âš ï¸ Epoch {epoch+1}: å‘ç°NaNå€¼ï¼Œè·³è¿‡æ‰¹æ¬¡")
                continue
            
            pred = model(xb)
            loss = loss_fn(pred, yb)
            
            if torch.isnan(loss):
                print(f"âš ï¸ Epoch {epoch+1}: Lossä¸ºNaNï¼Œè·³è¿‡æ‰¹æ¬¡")
                continue
            
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            total_loss += loss.item()
            batch_count += 1
        
        if batch_count > 0:
            avg_loss = total_loss / batch_count
            scheduler.step(avg_loss)
            
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
                torch.save(model.state_dict(), MODEL_PATH)
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}, Best: {best_loss:.6f}, Patience: {patience_counter}")
            
            # æ—©åœ
            if patience_counter >= max_patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                break
        else:
            print(f"Epoch {epoch+1}/{epochs}: æ‰€æœ‰æ‰¹æ¬¡éƒ½åŒ…å«NaNï¼Œè·³è¿‡")
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: {MODEL_PATH}")
    print(f"æœ€ç»ˆæŸå¤±: {best_loss:.6f}")

if __name__ == "__main__":
    train_improved(DATA_PATH) 