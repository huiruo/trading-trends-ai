# model/train_improved.py - æ”¹è¿›ç‰ˆè®­ç»ƒè„šæœ¬
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from preprocess import load_and_preprocess, create_sequences
from technical_indicators import add_technical_indicators
from model.model import LSTMModel
from config_improved import *
import numpy as np

def train_improved(csv_path: str, epochs=TRAIN_EPOCHS, lr=LEARNING_RATE, batch_size=BATCH_SIZE):
    # åŠ è½½æ•°æ®
    df = load_and_preprocess(csv_path)
    
    # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
    df = add_technical_indicators(df)
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰NaNæˆ–æ— ç©·å¤§
    print("æ£€æŸ¥æ•°æ®è´¨é‡...")
    nan_count = df.isna().sum().sum()
    inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()
    print(f"NaNå€¼æ•°é‡: {nan_count}")
    print(f"æ— ç©·å¤§å€¼æ•°é‡: {inf_count}")
    
    if nan_count > 0 or inf_count > 0:
        print("âš ï¸ å‘ç°NaNæˆ–æ— ç©·å¤§å€¼ï¼Œæ­£åœ¨æ¸…ç†...")
        # ç”¨å‰å‘å¡«å……å’Œåå‘å¡«å……æ¸…ç†NaN
        df = df.ffill().bfill()
        # ç”¨0æ›¿æ¢æ— ç©·å¤§å€¼
        df = df.replace([np.inf, -np.inf], 0)
        print("æ•°æ®æ¸…ç†å®Œæˆ")
    
    # åˆ›å»ºåºåˆ— - æ³¨æ„ï¼šè¿™é‡Œä¸ç¼©æ”¾ç›®æ ‡å€¼
    X, y = create_sequences(df, window_size=WINDOW_SIZE)
    
    print(f"CSV åŸå§‹Kçº¿æ•°æ®æ¡æ•°: {len(df)}")
    print(f"ç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬åºåˆ—æ•°: {len(X)}")
    print(f"ç‰¹å¾æ•°é‡: {X.shape[2]}")
    
    # æ£€æŸ¥è®­ç»ƒæ•°æ®
    print(f"X shape: {X.shape}, y shape: {y.shape}")
    print(f"X min: {np.min(X)}, X max: {np.max(X)}")
    print(f"y min: {np.min(y)}, y max: {np.max(y)}")
    print(f"Xä¸­æœ‰NaN: {np.isnan(X).any()}")
    print(f"yä¸­æœ‰NaN: {np.isnan(y).any()}")
    
    # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
    unique, counts = np.unique(y, return_counts=True)
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(unique, counts))}")
    print(f"æ ‡ç­¾æ¯”ä¾‹: è·Œ={counts[0]/len(y)*100:.1f}%, å¹³={counts[1]/len(y)*100:.1f}%, æ¶¨={counts[2]/len(y)*100:.1f}%")

    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)  # åˆ†ç±»æ ‡ç­¾ä½¿ç”¨longç±»å‹

    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
    model = LSTMModel(input_size=X.shape[2], hidden_size=64, num_layers=2, num_classes=3)

    # åˆ é™¤æ—§æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒ
    if os.path.exists(MODEL_PATH):
        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒ")
        os.remove(MODEL_PATH)

    # ä½¿ç”¨åˆ†ç±»æŸå¤±å‡½æ•°
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # æ·»åŠ L2æ­£åˆ™åŒ–
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    best_loss = float('inf')
    patience_counter = 0
    max_patience = 20
    
    for epoch in range(epochs):
        total_loss = 0
        batch_count = 0
        model.train()
        
        for xb, yb in dataloader:
            # æ£€æŸ¥æ‰¹æ¬¡æ•°æ®
            if torch.isnan(xb).any() or torch.isnan(yb).any():
                print(f"âš ï¸ Epoch {epoch+1}: å‘ç°NaNå€¼åœ¨æ‰¹æ¬¡æ•°æ®ä¸­")
                continue
                
            pred = model(xb)
            loss = loss_fn(pred, yb)
            
            # æ£€æŸ¥lossæ˜¯å¦ä¸ºNaN
            if torch.isnan(loss):
                print(f"âš ï¸ Epoch {epoch+1}: Lossä¸ºNaNï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
                
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            total_loss += loss.item()
            batch_count += 1

        if batch_count > 0:
            avg_loss = total_loss / batch_count
            scheduler.step(avg_loss)
            
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
                torch.save(model.state_dict(), MODEL_PATH)
            else:
                patience_counter += 1
            
            # æ¯10ä¸ªepochæ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.6f}, Best Loss: {best_loss:.6f}, Patience: {patience_counter}")
            
            # æ—©åœ
            if patience_counter >= max_patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                break
        else:
            print(f"Epoch {epoch+1}/{epochs}: æ‰€æœ‰æ‰¹æ¬¡éƒ½åŒ…å«NaNï¼Œè·³è¿‡æ­¤epoch")

    print(f"âœ… Improved model saved to {MODEL_PATH}")

if __name__ == "__main__":
    train_improved(DATA_PATH) 